{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Konkani NER Training - Google Colab\n",
    "\n",
    "Train custom Named Entity Recognition model for Konkani\n",
    "\n",
    "**Steps:**\n",
    "1. Install dependencies\n",
    "2. Mount Google Drive\n",
    "3. Upload code files\n",
    "4. Auto-label data (15 min)\n",
    "5. Train NER model (2-3 hours)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Cell 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Installing dependencies...\\n\")\n",
    "!pip install -q torch transformers pytorch-crf tqdm\n",
    "print(\"‚úÖ Dependencies installed!\\n\")\n",
    "\n",
    "# Verify GPU\n",
    "import torch\n",
    "print(f\"üîç GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Cell 2: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n‚úÖ Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ Cell 3: Upload Code & Data\n",
    "\n",
    "**Option A:** Upload from your Mac (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"üì§ Please upload these files from your Mac:\")\n",
    "print(\"   1. transcripts_konkani_cleaned.json\")\n",
    "print(\"   2. scripts/auto_label_ner.py\")\n",
    "print(\"   3. models/konkani_ner.py\")\n",
    "print(\"   4. train_konkani_ner.py\")\n",
    "print(\"\\nOr create a zip with all files and upload that.\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# If zip uploaded, extract it\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.zip'):\n",
    "        print(f\"\\nüìÇ Extracting {filename}...\")\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall('/content/')\n",
    "        print(\"‚úÖ Extracted!\")\n",
    "\n",
    "print(\"\\nüìã Files in /content/:\")\n",
    "!ls -la /content/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Cell 4: Auto-Label NER Data\n",
    "\n",
    "**This will take ~15 minutes for 2,500 samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"üè∑Ô∏è Starting auto-labeling...\\n\")\n",
    "print(\"This uses a pre-trained multilingual NER model to label your Konkani text.\")\n",
    "print(\"Expected time: ~15 minutes\\n\")\n",
    "\n",
    "!python3 /content/scripts/auto_label_ner.py \\\n",
    "    --input /content/transcripts_konkani_cleaned.json \\\n",
    "    --output /content/data/ner_labeled_data.json\n",
    "\n",
    "print(\"\\n‚úÖ Auto-labeling complete!\")\n",
    "print(\"\\nüìä Checking output...\")\n",
    "!ls -lh /content/data/ner_labeled_data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Cell 5: Train Custom NER Model\n",
    "\n",
    "**This will take ~2-3 hours on GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ STARTING NER TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nConfiguration:\")\n",
    "print(\"  ‚Ä¢ Device: CUDA (GPU)\")\n",
    "print(\"  ‚Ä¢ Batch size: 32\")\n",
    "print(\"  ‚Ä¢ Epochs: 20\")\n",
    "print(\"  ‚Ä¢ Model: BiLSTM-CRF\")\n",
    "print(\"  ‚Ä¢ Expected time: 2-3 hours\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "!python3 /content/train_konkani_ner.py \\\n",
    "    --data_file /content/data/ner_labeled_data.json \\\n",
    "    --batch_size 32 \\\n",
    "    --num_epochs 20 \\\n",
    "    --learning_rate 0.001 \\\n",
    "    --device cuda \\\n",
    "    --checkpoint_dir /content/checkpoints/ner \\\n",
    "    --use_crf\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Cell 6: Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Training Results\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüíæ Saved Models:\\n\")\n",
    "!ls -lh /content/checkpoints/ner/\n",
    "\n",
    "print(\"\\nüìà Model Size:\")\n",
    "!du -h /content/checkpoints/ner/best_ner_model.pt\n",
    "\n",
    "print(\"\\n‚úÖ Model ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Cell 7: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('/content')\n",
    "\n",
    "from models.konkani_ner import create_ner_model\n",
    "\n",
    "print(\"üß™ Testing NER Model\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load vocabularies\n",
    "with open('/content/checkpoints/ner/vocabularies.json', 'r') as f:\n",
    "    vocabs = json.load(f)\n",
    "    word2id = vocabs['word2id']\n",
    "    char2id = vocabs['char2id']\n",
    "\n",
    "# Load label map\n",
    "with open('/content/data/ner_labeled_data_label_map.json', 'r') as f:\n",
    "    label_map = json.load(f)\n",
    "    id2label = {int(k): v for k, v in label_map['id2label'].items()}\n",
    "\n",
    "# Create model\n",
    "model = create_ner_model(\n",
    "    vocab_size=len(word2id),\n",
    "    char_vocab_size=len(char2id),\n",
    "    num_tags=9,\n",
    "    use_crf=True\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "checkpoint = torch.load('/content/checkpoints/ner/best_ner_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "model = model.to('cuda')\n",
    "\n",
    "print(\"‚úÖ Model loaded!\\n\")\n",
    "\n",
    "# Test on sample text\n",
    "test_texts = [\n",
    "    \"‡§Æ‡•Ä ‡§Æ‡•Å‡§Ç‡§¨‡§à‡§Ç‡§§ ‡§ó‡•Ç‡§ó‡§≤‡§æ‡§Ç‡§§ ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ‡§Ç\",\n",
    "    \"‡§ó‡•ã‡§Ø‡§æ‡§Ç‡§§ ‡§ï‡§≤‡§Ç‡§ó‡•Å‡§ü ‡§¨‡•Ä‡§ö ‡§Ü‡§∏‡§æ\",\n",
    "    \"‡§Æ‡§æ‡§ù‡•á ‡§®‡§æ‡§µ ‡§∏‡•ç‡§ü‡§æ‡§µ‡§ø‡§® ‡§´‡§∞‡•ç‡§®‡§æ‡§Ç‡§°‡§ø‡§∏\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"\\nüìù Text: {text}\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Convert to IDs\n",
    "    word_ids = torch.tensor([[word2id.get(t, 1) for t in tokens]]).to('cuda')\n",
    "    \n",
    "    # Character IDs\n",
    "    char_ids = []\n",
    "    for token in tokens:\n",
    "        char_id_list = [char2id.get(c, 1) for c in token]\n",
    "        char_ids.append(char_id_list)\n",
    "    \n",
    "    max_char_len = max(len(chars) for chars in char_ids)\n",
    "    char_ids_padded = torch.zeros(1, len(tokens), max_char_len, dtype=torch.long).to('cuda')\n",
    "    for i, word_chars in enumerate(char_ids):\n",
    "        char_ids_padded[0, i, :len(word_chars)] = torch.tensor(word_chars, dtype=torch.long)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        predictions = model(word_ids, char_ids_padded)\n",
    "    \n",
    "    # Decode predictions\n",
    "    pred_labels = [id2label[p] for p in predictions[0]]\n",
    "    \n",
    "    # Show results\n",
    "    print(\"\\n   Entities found:\")\n",
    "    for token, label in zip(tokens, pred_labels):\n",
    "        if label != 'O':\n",
    "            print(f\"      {token:20s} ‚Üí {label}\")\n",
    "    \n",
    "    if all(label == 'O' for label in pred_labels):\n",
    "        print(\"      (No entities detected)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Cell 8: Backup to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backup_path = f\"/content/drive/MyDrive/konkanivani_training/ner_backup_{timestamp}\"\n",
    "\n",
    "print(f\"üíæ Backing up NER model to Google Drive...\\n\")\n",
    "print(f\"Backup location: {backup_path}\\n\")\n",
    "\n",
    "!mkdir -p {backup_path}\n",
    "!cp -r /content/checkpoints/ner/* {backup_path}/\n",
    "!cp /content/data/ner_labeled_data* {backup_path}/\n",
    "\n",
    "print(\"\\n‚úÖ Backup complete!\\n\")\n",
    "print(\"üìã Backed up files:\\n\")\n",
    "!ls -lh {backup_path}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Cell 9: Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "print(\"üì¶ Creating download package...\\n\")\n",
    "\n",
    "# Create zip\n",
    "!cd /content && zip -r ner_model.zip checkpoints/ner/ data/ner_labeled_data*\n",
    "\n",
    "print(\"\\nüìä Package size:\")\n",
    "!ls -lh /content/ner_model.zip\n",
    "\n",
    "print(\"\\nüì• Downloading...\")\n",
    "files.download('/content/ner_model.zip')\n",
    "\n",
    "print(\"\\n‚úÖ Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "After running all cells, you'll have:\n",
    "\n",
    "1. ‚úÖ Auto-labeled NER dataset (~2,500 samples)\n",
    "2. ‚úÖ Trained custom NER model (BiLSTM-CRF)\n",
    "3. ‚úÖ Model backed up to Google Drive\n",
    "4. ‚úÖ Model downloaded to your computer\n",
    "\n",
    "**Next steps:**\n",
    "- Integrate NER into your complete audio analyzer\n",
    "- Test with real Konkani audio\n",
    "- Deploy to Hugging Face Spaces\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
