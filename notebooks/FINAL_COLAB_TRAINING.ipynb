{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KonkaniVani ASR Training - Final Setup\n",
    "## Resume from Checkpoint 15 - Memory Optimized for Tesla T4\n",
    "\n",
    "**Drive Folder**: https://drive.google.com/drive/u/5/folders/1KX7k_z2negFKq3qFjHJh-K1U-MEcNp7P\n",
    "\n",
    "**Configuration:**\n",
    "- Model: d_model=256, 12 encoder, 6 decoder layers (from checkpoint 15)\n",
    "- Batch size: 2 + gradient accumulation 4x = effective batch 8\n",
    "- Mixed precision: FP16 (saves ~50% GPU memory)\n",
    "- Resume from: Epoch 15 ‚Üí Train to Epoch 50\n",
    "- Expected time: 8-12 hours on Tesla T4\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "print(\"\\n‚ö†Ô∏è Make sure you see 'Tesla T4' above!\")\n",
    "print(\"If not: Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì¶ Installing dependencies...\\n\")\n",
    "!pip install -q torch torchaudio librosa soundfile tensorboard tqdm pyyaml\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"\\n‚úÖ Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Locate and Copy Project Files\n",
    "\n",
    "**Your Drive folder ID**: `1KX7k_z2negFKq3qFjHJh-K1U-MEcNp7P`\n",
    "\n",
    "First, let's find where your files are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç Searching for your project files...\\n\")\n",
    "\n",
    "# Common Drive locations\n",
    "search_paths = [\n",
    "    \"/content/drive/MyDrive\",\n",
    "    \"/content/drive/Shareddrives\",\n",
    "]\n",
    "\n",
    "# Look for the project\n",
    "for base_path in search_paths:\n",
    "    if os.path.exists(base_path):\n",
    "        print(f\"üìÇ Checking: {base_path}\")\n",
    "        !ls -la {base_path} | head -20\n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"üëÜ Look for your project folder or zip file above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Extract/Copy Project\n",
    "\n",
    "**Update the path below** based on what you saw in Step 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================\n",
    "# UPDATE THESE PATHS BASED ON YOUR DRIVE\n",
    "# ============================================\n",
    "\n",
    "# Option A: If you have a ZIP file\n",
    "USE_ZIP = True\n",
    "ZIP_PATH = \"/content/drive/MyDrive/konkani_project.zip\"  # UPDATE THIS\n",
    "\n",
    "# Option B: If you have a folder\n",
    "FOLDER_PATH = \"/content/drive/MyDrive/konkani\"  # UPDATE THIS\n",
    "\n",
    "# ============================================\n",
    "\n",
    "%cd /content\n",
    "\n",
    "if USE_ZIP:\n",
    "    print(f\"üì¶ Extracting from: {ZIP_PATH}\\n\")\n",
    "    if Path(ZIP_PATH).exists():\n",
    "        !unzip -q {ZIP_PATH} -d /content/\n",
    "        print(\"‚úÖ Extracted!\\n\")\n",
    "        !ls -la /content/\n",
    "    else:\n",
    "        print(f\"‚ùå ZIP not found at: {ZIP_PATH}\")\n",
    "        print(\"\\nüìù Please update ZIP_PATH above with the correct path from Step 4\")\n",
    "else:\n",
    "    print(f\"üìã Copying from: {FOLDER_PATH}\\n\")\n",
    "    if Path(FOLDER_PATH).exists():\n",
    "        !cp -r {FOLDER_PATH} /content/konkani\n",
    "        print(\"‚úÖ Copied!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Folder not found at: {FOLDER_PATH}\")\n",
    "        print(\"\\nüìù Please update FOLDER_PATH above with the correct path from Step 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Navigate to Project Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Try to find the project directory automatically\n",
    "possible_dirs = [\n",
    "    '/content/konkani',\n",
    "    '/content/konkani_project',\n",
    "    '/content',\n",
    "]\n",
    "\n",
    "project_dir = None\n",
    "for dir_path in possible_dirs:\n",
    "    check_file = f\"{dir_path}/training_scripts/train_konkanivani_asr.py\"\n",
    "    if os.path.exists(check_file):\n",
    "        project_dir = dir_path\n",
    "        break\n",
    "\n",
    "if project_dir:\n",
    "    print(f\"‚úÖ Found project at: {project_dir}\\n\")\n",
    "    %cd {project_dir}\n",
    "    !pwd\n",
    "    print(\"\\nProject contents:\")\n",
    "    !ls -la\n",
    "else:\n",
    "    print(\"‚ùå Could not find project directory automatically.\")\n",
    "    print(\"\\nPlease check the extracted folder name:\")\n",
    "    !ls -la /content/\n",
    "    print(\"\\nüìù Then manually set the path:\")\n",
    "    print(\"   %cd /content/[your_folder_name]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Verify All Required Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "required_files = [\n",
    "    'training_scripts/train_konkanivani_asr.py',\n",
    "    'models/konkanivani_asr.py',\n",
    "    'data/audio_processing/dataset.py',\n",
    "    'data/audio_processing/text_tokenizer.py',\n",
    "    'data/vocab.json',\n",
    "    'data/konkani-asr-v0/splits/manifests/train.json',\n",
    "    'data/konkani-asr-v0/splits/manifests/val.json',\n",
    "    'archives/checkpoint_epoch_15.pt'\n",
    "]\n",
    "\n",
    "print(\"Checking required files...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_good = True\n",
    "for file in required_files:\n",
    "    exists = os.path.exists(file)\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"{status} {file}\")\n",
    "    if not exists:\n",
    "        all_good = False\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\nüéâ All files found! Ready to train!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some files are missing. Please check your Drive folder.\")\n",
    "    print(\"\\nMake sure your Drive folder contains:\")\n",
    "    print(\"  ‚Ä¢ training_scripts/\")\n",
    "    print(\"  ‚Ä¢ models/\")\n",
    "    print(\"  ‚Ä¢ data/ (with audio files and manifests)\")\n",
    "    print(\"  ‚Ä¢ archives/checkpoint_epoch_15.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Prepare Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints\n",
    "!cp archives/checkpoint_epoch_15.pt checkpoints/\n",
    "\n",
    "print(\"‚úÖ Checkpoint copied to checkpoints/\\n\")\n",
    "!ls -lh checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Verify Checkpoint Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "print(\"üìã Loading checkpoint...\\n\")\n",
    "checkpoint = torch.load('checkpoints/checkpoint_epoch_15.pt', map_location='cpu')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CHECKPOINT CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(checkpoint.get('config', {}), indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "state = checkpoint['model_state_dict']\n",
    "encoder_layers = sum(1 for k in state.keys() if 'encoder.layers.' in k and '.ff1.0.weight' in k)\n",
    "decoder_layers = sum(1 for k in state.keys() if 'decoder.decoder.layers.' in k and '.linear1.weight' in k)\n",
    "d_model = state['encoder.input_proj.weight'].shape[0]\n",
    "vocab_size = state['ctc_head.weight'].shape[0]\n",
    "\n",
    "print(f\"Encoder layers:  {encoder_layers}\")\n",
    "print(f\"Decoder layers:  {decoder_layers}\")\n",
    "print(f\"d_model:         {d_model}\")\n",
    "print(f\"vocab_size:      {vocab_size}\")\n",
    "print(f\"Last epoch:      {checkpoint['epoch']}\")\n",
    "print(f\"Val loss:        {checkpoint.get('val_loss', 'N/A')}\")\n",
    "\n",
    "# Calculate model size\n",
    "num_params = sum(p.numel() for p in [torch.zeros(v.shape) for v in state.values()])\n",
    "print(f\"Parameters:      {num_params:,}\")\n",
    "\n",
    "del checkpoint\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n‚úÖ Checkpoint verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Setup Environment & Clear GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Set environment variables for memory optimization\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "\n",
    "# Clear GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GPU STATUS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU:             {torch.cuda.get_device_name(0)}\")\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    cached = torch.cuda.memory_reserved(0) / 1e9\n",
    "    free = total_mem - allocated - cached\n",
    "    \n",
    "    print(f\"Total memory:    {total_mem:.2f} GB\")\n",
    "    print(f\"Allocated:       {allocated:.2f} GB\")\n",
    "    print(f\"Cached:          {cached:.2f} GB\")\n",
    "    print(f\"Free:            {free:.2f} GB\")\n",
    "    \n",
    "    print(\"\\n‚úÖ GPU ready for training!\")\n",
    "    \n",
    "    if total_mem < 14:\n",
    "        print(\"\\n‚ö†Ô∏è Warning: GPU has less than 14GB. You may need batch_size=1\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not available!\")\n",
    "    print(\"\\nPlease set GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: üöÄ START TRAINING\n",
    "\n",
    "### Memory-Optimized Configuration:\n",
    "\n",
    "| Setting | Value | Purpose |\n",
    "|---------|-------|----------|\n",
    "| Batch size | 2 | Fits in 14GB GPU |\n",
    "| Gradient accumulation | 4 | Effective batch = 8 |\n",
    "| Mixed precision | FP16 | Saves ~50% memory |\n",
    "| d_model | 256 | From checkpoint |\n",
    "| Encoder layers | 12 | From checkpoint |\n",
    "| Decoder layers | 6 | From checkpoint |\n",
    "| Resume from | Epoch 15 | Continue training |\n",
    "| Target | Epoch 50 | 35 more epochs |\n",
    "\n",
    "**Expected time**: ~8-12 hours\n",
    "\n",
    "**Keep this tab open!** Colab disconnects after 90 minutes of inactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üöÄ STARTING KONKANIVANI ASR TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(\"Configuration:\")\n",
    "print(\"  ‚Ä¢ Batch size: 2 (gradient accumulation: 4x)\")\n",
    "print(\"  ‚Ä¢ Mixed precision: FP16\")\n",
    "print(\"  ‚Ä¢ Model: d_model=256, 12 encoder, 6 decoder layers\")\n",
    "print(\"  ‚Ä¢ Resume from: Epoch 15\")\n",
    "print(\"  ‚Ä¢ Target: Epoch 50\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚è∞ Estimated time: 8-12 hours\")\n",
    "print(\"‚ö†Ô∏è  Keep this tab open to prevent disconnection!\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n\")\n",
    "\n",
    "!python3 training_scripts/train_konkanivani_asr.py \\\n",
    "    --train_manifest data/konkani-asr-v0/splits/manifests/train.json \\\n",
    "    --val_manifest data/konkani-asr-v0/splits/manifests/val.json \\\n",
    "    --vocab_file data/vocab.json \\\n",
    "    --batch_size 2 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --num_epochs 50 \\\n",
    "    --learning_rate 0.0005 \\\n",
    "    --device cuda \\\n",
    "    --d_model 256 \\\n",
    "    --encoder_layers 12 \\\n",
    "    --decoder_layers 6 \\\n",
    "    --mixed_precision \\\n",
    "    --checkpoint_dir checkpoints \\\n",
    "    --log_dir logs \\\n",
    "    --resume checkpoints/checkpoint_epoch_15.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Monitor GPU (Run While Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "print(\"\\nüìä Expected values during training:\")\n",
    "print(\"  ‚Ä¢ GPU Utilization: 80-95%\")\n",
    "print(\"  ‚Ä¢ Memory Used: ~7-8 GB / 14 GB\")\n",
    "print(\"  ‚Ä¢ Temperature: 60-80¬∞C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: View TensorBoard Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Backup to Drive (Run Every Few Hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "BACKUP_PATH = \"/content/drive/MyDrive/konkanivani_backup\"\n",
    "\n",
    "print(f\"üì§ Backing up to: {BACKUP_PATH}\")\n",
    "print(f\"   Time: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "!mkdir -p {BACKUP_PATH}\n",
    "!cp -r checkpoints {BACKUP_PATH}/checkpoints_$(date +%Y%m%d_%H%M%S)\n",
    "!cp -r logs {BACKUP_PATH}/logs_$(date +%Y%m%d_%H%M%S)\n",
    "\n",
    "print(\"\\n‚úÖ Backup completed!\")\n",
    "!ls -lh {BACKUP_PATH}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Download Best Model (After Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Available checkpoints:\\n\")\n",
    "!ls -lh checkpoints/\n",
    "\n",
    "if Path('checkpoints/best_model.pt').exists():\n",
    "    print(\"\\nüì• Downloading best_model.pt...\")\n",
    "    files.download('checkpoints/best_model.pt')\n",
    "    print(\"‚úÖ Downloaded!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è best_model.pt not found yet. Training may still be in progress.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üÜò Emergency: Out of Memory\n",
    "\n",
    "If you get OOM error, run this cell instead of Step 11:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory\n",
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"üîß Running with MINIMAL memory settings:\")\n",
    "print(\"  ‚Ä¢ Batch size: 1 (reduced from 2)\")\n",
    "print(\"  ‚Ä¢ Gradient accumulation: 8 (maintains effective batch = 8)\")\n",
    "print(\"\\n‚è∞ This will be slower but should fit in memory\\n\")\n",
    "\n",
    "!python3 training_scripts/train_konkanivani_asr.py \\\n",
    "    --train_manifest data/konkani-asr-v0/splits/manifests/train.json \\\n",
    "    --val_manifest data/konkani-asr-v0/splits/manifests/val.json \\\n",
    "    --vocab_file data/vocab.json \\\n",
    "    --batch_size 1 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --num_epochs 50 \\\n",
    "    --learning_rate 0.0005 \\\n",
    "    --device cuda \\\n",
    "    --d_model 256 \\\n",
    "    --encoder_layers 12 \\\n",
    "    --decoder_layers 6 \\\n",
    "    --mixed_precision \\\n",
    "    --checkpoint_dir checkpoints \\\n",
    "    --log_dir logs \\\n",
    "    --resume checkpoints/checkpoint_epoch_15.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Quick Reference\n",
    "\n",
    "### Check GPU\n",
    "```python\n",
    "!nvidia-smi\n",
    "```\n",
    "\n",
    "### Clear Memory\n",
    "```python\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "### List Checkpoints\n",
    "```python\n",
    "!ls -lh checkpoints/\n",
    "```\n",
    "\n",
    "### Resume from Different Checkpoint\n",
    "```python\n",
    "# In Step 11, change:\n",
    "--resume checkpoints/checkpoint_epoch_20.pt\n",
    "```\n",
    "\n",
    "### Check Training Progress\n",
    "```python\n",
    "!tail -50 logs/events.out.tfevents.*\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Success Indicators\n",
    "\n",
    "- Training starts without OOM error\n",
    "- GPU memory stable at ~7-8 GB\n",
    "- Batch processing: 2-3 batches/sec\n",
    "- Loss decreasing over epochs\n",
    "- Checkpoints saving every 5 epochs\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ After Training\n",
    "\n",
    "1. Run Step 14 to backup everything\n",
    "2. Run Step 15 to download best model\n",
    "3. Evaluate on test set\n",
    "4. Deploy for inference\n",
    "\n",
    "Good luck! üöÄ\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
