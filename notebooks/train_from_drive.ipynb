{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KonkaniVani ASR Training - From Google Drive\n",
    "## Resume from Checkpoint 15\n",
    "\n",
    "**Drive Folder**: https://drive.google.com/drive/folders/1-chxczmcNooqLDtsFgQ8ZT8NvzFuFARr\n",
    "\n",
    "**Configuration:**\n",
    "- Model: d_model=256, 12 encoder, 6 decoder layers\n",
    "- Batch size: 2 (gradient accumulation 4x = effective batch 8)\n",
    "- Mixed precision: FP16\n",
    "- GPU: Tesla T4 (14GB)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchaudio librosa soundfile tensorboard tqdm pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Locate Your Project Files\n",
    "\n",
    "**IMPORTANT**: Update the path below to match where your files are in the shared Drive folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# List files in the shared folder to find the correct path\n",
    "print(\"üìÇ Checking Drive structure...\\n\")\n",
    "\n",
    "# Try common locations\n",
    "possible_paths = [\n",
    "    \"/content/drive/MyDrive/konkani\",\n",
    "    \"/content/drive/Shareddrives/*/konkani\",\n",
    "    \"/content/drive/MyDrive\",\n",
    "]\n",
    "\n",
    "# List what's in MyDrive\n",
    "!ls -la /content/drive/MyDrive/ | head -20\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üëÜ Look for your project folder name above\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set Project Path and Copy to Colab\n",
    "\n",
    "**IMPORTANT**: First upload your project to Google Drive!\n",
    "\n",
    "### Option A: Upload as ZIP\n",
    "1. On your local machine: `zip -r konkani_project.zip .`\n",
    "2. Upload to Google Drive\n",
    "3. Update path below\n",
    "\n",
    "### Option B: Upload folder directly\n",
    "1. Upload entire `konkani` folder to Drive\n",
    "2. Update path below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ========================================\n",
    "# OPTION A: Extract from ZIP file\n",
    "# ========================================\n",
    "USE_ZIP = True  # Set to True if using zip file\n",
    "ZIP_PATH = \"/content/drive/MyDrive/konkani_project.zip\"  # Update this\n",
    "\n",
    "# ========================================\n",
    "# OPTION B: Copy from folder\n",
    "# ========================================\n",
    "DRIVE_FOLDER_PATH = \"/content/drive/MyDrive/konkani\"  # Update this\n",
    "\n",
    "# ========================================\n",
    "\n",
    "%cd /content\n",
    "\n",
    "if USE_ZIP:\n",
    "    print(f\"üì¶ Extracting from: {ZIP_PATH}\")\n",
    "    if Path(ZIP_PATH).exists():\n",
    "        !unzip -q {ZIP_PATH} -d /content/\n",
    "        # Find the extracted folder\n",
    "        !ls -la /content/\n",
    "        print(\"\\n‚úÖ Extracted! Check folder name above and update next cell if needed.\")\n",
    "    else:\n",
    "        print(f\"‚ùå ZIP file not found at: {ZIP_PATH}\")\n",
    "        print(\"\\nüìù Please:\")\n",
    "        print(\"   1. Create zip: zip -r konkani_project.zip .\")\n",
    "        print(\"   2. Upload to Google Drive\")\n",
    "        print(\"   3. Update ZIP_PATH above\")\n",
    "else:\n",
    "    print(f\"üìã Copying from: {DRIVE_FOLDER_PATH}\")\n",
    "    if Path(DRIVE_FOLDER_PATH).exists():\n",
    "        !cp -r {DRIVE_FOLDER_PATH} /content/konkani\n",
    "        print(\"‚úÖ Copied to /content/konkani\")\n",
    "    else:\n",
    "        print(f\"‚ùå Folder not found at: {DRIVE_FOLDER_PATH}\")\n",
    "        print(\"\\nüìù Please upload your project folder to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Navigate to Project Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this if your extracted folder has a different name\n",
    "PROJECT_DIR = \"/content/konkani\"  # or \"/content/konkani_project\" etc.\n",
    "\n",
    "%cd {PROJECT_DIR}\n",
    "!pwd\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Required Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "required_files = [\n",
    "    'training_scripts/train_konkanivani_asr.py',\n",
    "    'models/konkanivani_asr.py',\n",
    "    'data/audio_processing/dataset.py',\n",
    "    'data/audio_processing/text_tokenizer.py',\n",
    "    'data/vocab.json',\n",
    "    'data/konkani-asr-v0/splits/manifests/train.json',\n",
    "    'data/konkani-asr-v0/splits/manifests/val.json',\n",
    "    'archives/checkpoint_epoch_15.pt'\n",
    "]\n",
    "\n",
    "print(\"Checking required files...\\n\")\n",
    "all_good = True\n",
    "for file in required_files:\n",
    "    exists = os.path.exists(file)\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"{status} {file}\")\n",
    "    if not exists:\n",
    "        all_good = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all_good:\n",
    "    print(\"‚úÖ All required files found! Ready to train.\")\n",
    "else:\n",
    "    print(\"‚ùå Some files are missing. Please check your Drive folder.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints\n",
    "!cp archives/checkpoint_epoch_15.pt checkpoints/\n",
    "!ls -lh checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Verify Checkpoint Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "checkpoint = torch.load('checkpoints/checkpoint_epoch_15.pt', map_location='cpu')\n",
    "\n",
    "print(\"üìã Checkpoint Configuration:\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(checkpoint.get('config', {}), indent=2))\n",
    "\n",
    "print(\"\\nüìä Model Architecture:\")\n",
    "print(\"=\"*60)\n",
    "state = checkpoint['model_state_dict']\n",
    "encoder_layers = sum(1 for k in state.keys() if 'encoder.layers.' in k and '.ff1.0.weight' in k)\n",
    "decoder_layers = sum(1 for k in state.keys() if 'decoder.decoder.layers.' in k and '.linear1.weight' in k)\n",
    "d_model = state['encoder.input_proj.weight'].shape[0]\n",
    "vocab_size = state['ctc_head.weight'].shape[0]\n",
    "\n",
    "print(f\"Encoder layers: {encoder_layers}\")\n",
    "print(f\"Decoder layers: {decoder_layers}\")\n",
    "print(f\"d_model: {d_model}\")\n",
    "print(f\"vocab_size: {vocab_size}\")\n",
    "print(f\"Epoch: {checkpoint['epoch']}\")\n",
    "print(f\"Val loss: {checkpoint.get('val_loss', 'N/A')}\")\n",
    "\n",
    "del checkpoint\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n‚úÖ Checkpoint verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "\n",
    "print(\"‚úÖ Environment variables set for memory optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Clear GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"Cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "    print(\"\\n‚úÖ Ready to train!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA not available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. üöÄ START TRAINING\n",
    "\n",
    "### Memory-Optimized Configuration:\n",
    "- **Batch size**: 2 (reduced from 8)\n",
    "- **Gradient accumulation**: 4 steps (effective batch = 8)\n",
    "- **Mixed precision**: FP16\n",
    "- **Model**: d_model=256, 12 encoder, 6 decoder layers\n",
    "- **Resume from**: Epoch 15\n",
    "\n",
    "**Expected time**: ~8-12 hours for epochs 16-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 training_scripts/train_konkanivani_asr.py \\\n",
    "    --train_manifest data/konkani-asr-v0/splits/manifests/train.json \\\n",
    "    --val_manifest data/konkani-asr-v0/splits/manifests/val.json \\\n",
    "    --vocab_file data/vocab.json \\\n",
    "    --batch_size 2 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --num_epochs 50 \\\n",
    "    --learning_rate 0.0005 \\\n",
    "    --device cuda \\\n",
    "    --d_model 256 \\\n",
    "    --encoder_layers 12 \\\n",
    "    --decoder_layers 6 \\\n",
    "    --mixed_precision \\\n",
    "    --checkpoint_dir checkpoints \\\n",
    "    --log_dir logs \\\n",
    "    --resume checkpoints/checkpoint_epoch_15.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Monitor GPU (Run in Separate Cell While Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. View TensorBoard Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Backup to Google Drive (Run Every Few Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Set your backup location\n",
    "BACKUP_PATH = \"/content/drive/MyDrive/konkanivani_backup\"\n",
    "\n",
    "print(f\"üì§ Backing up to: {BACKUP_PATH}\")\n",
    "print(f\"   Time: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "!mkdir -p {BACKUP_PATH}\n",
    "!cp -r checkpoints {BACKUP_PATH}/\n",
    "!cp -r logs {BACKUP_PATH}/\n",
    "\n",
    "print(\"\\n‚úÖ Backup completed!\")\n",
    "!ls -lh {BACKUP_PATH}/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. If Out of Memory - Use This Instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory first\n",
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Run with batch_size=1 (slower but uses less memory)\n",
    "!python3 training_scripts/train_konkanivani_asr.py \\\n",
    "    --train_manifest data/konkani-asr-v0/splits/manifests/train.json \\\n",
    "    --val_manifest data/konkani-asr-v0/splits/manifests/val.json \\\n",
    "    --vocab_file data/vocab.json \\\n",
    "    --batch_size 1 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --num_epochs 50 \\\n",
    "    --learning_rate 0.0005 \\\n",
    "    --device cuda \\\n",
    "    --d_model 256 \\\n",
    "    --encoder_layers 12 \\\n",
    "    --decoder_layers 6 \\\n",
    "    --mixed_precision \\\n",
    "    --checkpoint_dir checkpoints \\\n",
    "    --log_dir logs \\\n",
    "    --resume checkpoints/checkpoint_epoch_15.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Download Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# List all checkpoints\n",
    "!ls -lh checkpoints/\n",
    "\n",
    "# Download best model\n",
    "if Path('checkpoints/best_model.pt').exists():\n",
    "    print(\"\\nüì• Downloading best_model.pt...\")\n",
    "    files.download('checkpoints/best_model.pt')\n",
    "    print(\"‚úÖ Downloaded!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è best_model.pt not found yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Troubleshooting\n",
    "\n",
    "### Out of Memory\n",
    "```python\n",
    "# Run cell 15 instead (batch_size=1)\n",
    "```\n",
    "\n",
    "### Files Not Found\n",
    "```python\n",
    "# Update DRIVE_PROJECT_PATH in cell 5\n",
    "# Make sure all files are in your Drive folder\n",
    "```\n",
    "\n",
    "### Session Disconnected\n",
    "```python\n",
    "# Resume from latest checkpoint\n",
    "# Change --resume to latest checkpoint_epoch_XX.pt\n",
    "```\n",
    "\n",
    "### Slow Training\n",
    "```python\n",
    "# Check GPU is being used\n",
    "!nvidia-smi\n",
    "# Should show ~80-95% GPU utilization\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
