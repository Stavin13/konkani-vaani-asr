{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Translation Pairs for Konkani-English\n",
    "\n",
    "**Strategy:** Use pre-trained translation model to create Konkani-English pairs  \n",
    "**Then:** Train custom Konkani-specific translation model on this data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch pandas tqdm sentencepiece\n",
    "print(\"✅ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Pre-trained Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Use multilingual translation model\n",
    "# This model can handle Indic languages including Konkani\n",
    "translator = pipeline(\n",
    "    \"translation\",\n",
    "    model=\"Helsinki-NLP/opus-mt-mul-en\",  # Multilingual to English\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"✅ Pre-trained translation model loaded!\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Konkani Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load your Konkani text from various sources\n",
    "konkani_texts = []\n",
    "\n",
    "# Option 1: From ASR transcripts\n",
    "try:\n",
    "    with open('data/konkani-asr-v0/splits/manifests/train.json', 'r') as f:\n",
    "        asr_data = [json.loads(line) for line in f]\n",
    "        konkani_texts.extend([item['text'] for item in asr_data])\n",
    "    print(f\"✅ Loaded {len(asr_data)} texts from ASR\")\n",
    "except:\n",
    "    print(\"⚠️ ASR data not found\")\n",
    "\n",
    "# Option 2: From NER data\n",
    "try:\n",
    "    with open('data/generated/konkani_large_dataset.json', 'r') as f:\n",
    "        ner_data = json.load(f)\n",
    "        konkani_texts.extend([item['text'] for item in ner_data])\n",
    "    print(f\"✅ Loaded {len(ner_data)} texts from NER data\")\n",
    "except:\n",
    "    print(\"⚠️ NER data not found\")\n",
    "\n",
    "# Option 3: From custom corpus\n",
    "# with open('your_konkani_corpus.txt', 'r') as f:\n",
    "#     konkani_texts.extend(f.readlines())\n",
    "\n",
    "# Remove duplicates and clean\n",
    "konkani_texts = list(set([t.strip() for t in konkani_texts if t.strip()]))\n",
    "\n",
    "print(f\"\\nTotal unique Konkani texts: {len(konkani_texts)}\")\n",
    "print(f\"\\nSamples:\")\n",
    "for i in range(min(5, len(konkani_texts))):\n",
    "    print(f\"  {i+1}. {konkani_texts[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate English Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Generate translations in batches\n",
    "batch_size = 16\n",
    "translation_pairs = []\n",
    "\n",
    "print(\"Generating English translations...\\n\")\n",
    "\n",
    "for i in tqdm(range(0, len(konkani_texts), batch_size)):\n",
    "    batch = konkani_texts[i:i+batch_size]\n",
    "    \n",
    "    try:\n",
    "        # Translate batch\n",
    "        results = translator(batch, max_length=128)\n",
    "        \n",
    "        # Store pairs\n",
    "        for konkani, result in zip(batch, results):\n",
    "            translation_pairs.append({\n",
    "                'konkani': konkani,\n",
    "                'english': result['translation_text']\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️ Error in batch {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Generated {len(translation_pairs)} translation pairs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Quality Check & Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out poor quality translations\n",
    "def is_valid_translation(pair):\n",
    "    konkani = pair['konkani']\n",
    "    english = pair['english']\n",
    "    \n",
    "    # Basic quality checks\n",
    "    if len(english.split()) < 2:  # Too short\n",
    "        return False\n",
    "    if len(english) > len(konkani) * 3:  # Too long\n",
    "        return False\n",
    "    if english == konkani:  # Not translated\n",
    "        return False\n",
    "    if not any(c.isalpha() for c in english):  # No letters\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Filter\n",
    "filtered_pairs = [p for p in translation_pairs if is_valid_translation(p)]\n",
    "\n",
    "print(f\"Total pairs: {len(translation_pairs)}\")\n",
    "print(f\"Valid pairs: {len(filtered_pairs)}\")\n",
    "print(f\"Filtered out: {len(translation_pairs) - len(filtered_pairs)}\")\n",
    "\n",
    "# Show statistics\n",
    "df = pd.DataFrame(filtered_pairs)\n",
    "print(f\"\\nAverage Konkani length: {df['konkani'].str.len().mean():.1f} chars\")\n",
    "print(f\"Average English length: {df['english'].str.len().mean():.1f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Add Reverse Translations (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally add reverse translations (English → Konkani)\n",
    "# This helps create bidirectional translation capability\n",
    "\n",
    "print(\"Generating reverse translations (English → Konkani)...\\n\")\n",
    "\n",
    "# Load reverse translator\n",
    "reverse_translator = pipeline(\n",
    "    \"translation\",\n",
    "    model=\"Helsinki-NLP/opus-mt-en-mul\",  # English to Multilingual\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Take a subset of English texts to translate back\n",
    "english_texts = [p['english'] for p in filtered_pairs[:1000]]  # Limit for speed\n",
    "\n",
    "reverse_pairs = []\n",
    "for i in tqdm(range(0, len(english_texts), batch_size)):\n",
    "    batch = english_texts[i:i+batch_size]\n",
    "    \n",
    "    try:\n",
    "        results = reverse_translator(batch, max_length=128)\n",
    "        \n",
    "        for english, result in zip(batch, results):\n",
    "            reverse_pairs.append({\n",
    "                'konkani': result['translation_text'],\n",
    "                'english': english\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️ Error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Generated {len(reverse_pairs)} reverse pairs!\")\n",
    "\n",
    "# Combine\n",
    "all_pairs = filtered_pairs + reverse_pairs\n",
    "print(f\"Total translation pairs: {len(all_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('data/generated', exist_ok=True)\n",
    "\n",
    "# Save as JSON\n",
    "with open('data/generated/konkani_english_translation_pairs.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_pairs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save as CSV for easy viewing\n",
    "df_all = pd.DataFrame(all_pairs)\n",
    "df_all.to_csv('data/generated/konkani_english_translation_pairs.csv', index=False)\n",
    "\n",
    "print(\"✅ Saved translation data:\")\n",
    "print(\"   - data/generated/konkani_english_translation_pairs.json\")\n",
    "print(\"   - data/generated/konkani_english_translation_pairs.csv\")\n",
    "\n",
    "print(f\"\\nReady for training with {len(all_pairs)} translation pairs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Verify Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample translations\n",
    "print(\"=\"*70)\n",
    "print(\"SAMPLE TRANSLATION PAIRS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import random\n",
    "samples = random.sample(all_pairs, min(10, len(all_pairs)))\n",
    "\n",
    "for i, pair in enumerate(samples, 1):\n",
    "    print(f\"\\n{i}.\")\n",
    "    print(f\"   Konkani: {pair['konkani']}\")\n",
    "    print(f\"   English: {pair['english']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ DATA GENERATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext step: Upload this data to Kaggle and train custom translation model!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
